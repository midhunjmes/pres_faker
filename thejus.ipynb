{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/midhunjmes/pres_faker/blob/main/thejus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install openpyxl\n",
        "!pip install presidio_analyzer\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "gP5BWVM0NAh1",
        "outputId": "a40281e1-dca4-48bc-f8f2-6cd4dec95e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting presidio_analyzer\n",
            "  Downloading presidio_analyzer-2.2.358-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting phonenumbers<9.0.0,>=8.12 (from presidio_analyzer)\n",
            "  Downloading phonenumbers-8.13.55-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (2024.11.6)\n",
            "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (3.8.4)\n",
            "Collecting tldextract (from presidio_analyzer)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.10)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio_analyzer)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.18.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.2)\n",
            "Downloading presidio_analyzer-2.2.358-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-8.13.55-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: phonenumbers, requests-file, tldextract, presidio_analyzer\n",
            "Successfully installed phonenumbers-8.13.55 presidio_analyzer-2.2.358 requests-file-2.1.0 tldextract-5.1.3\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# analyzer = AnalyzerEngine()\n",
        "# for recognizer in analyzer.get_recognizers():\n",
        "#     print(recognizer.supported_entities)"
      ],
      "metadata": {
        "id": "clvNKuSWc9QN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import openpyxl\n",
        "import json\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "analyzer = AnalyzerEngine()\n",
        "mapping={}\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "#function to find out the noun values dominating columns considering it will be a sensitive data if there is so many nouns\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "def detect_noun(file_path):\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    sensitive = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            continue  # Skip purely numeric columns\n",
        "\n",
        "        text_samples = df[col].astype(str).head(5)  # Take first 5 values\n",
        "        noun_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for value in text_samples:\n",
        "            if \"%\" in value or value.replace(\".\", \"\").isdigit():\n",
        "                continue  # Skip percentage or number-like values\n",
        "\n",
        "            doc = nlp(value)\n",
        "            for token in doc:\n",
        "                if token.pos_ in ['NOUN', 'PROPN']:\n",
        "                    noun_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "        # Mark column as sensitive only if a significant portion are nouns\n",
        "        if total_count > 0 and (noun_count / total_count) > 0.2:\n",
        "            sensitive.append(col)\n",
        "\n",
        "    return sensitive\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "#finding out the descriptive data columns which may have sensitive data in the form of text\n",
        "#------------------------------------------------------------------------------------------\n",
        "def descriptive_columns(file_path):\n",
        "    # Define keywords to filter out\n",
        "    keywords = [\"description\", \"remarks\", \"notes\", \"comments\", \"observations\", \"details\", \"summary\", \"explanation\",\n",
        "    \"reviews\", \"feedback\", \"testimonials\", \"opinions\", \"assessment\", \"suggestions\", \"experience\",\n",
        "    \"incident_report\", \"case_notes\", \"audit_notes\", \"findings\", \"status_update\", \"history\", \"progress_report\",\n",
        "    \"additional_info\", \"clarifications\", \"justification\", \"annotations\", \"excerpts\", \"statement\", \"explanation_text\"]\n",
        "\n",
        "    # Ensure columns are properly loaded from CSV/Excel\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, nrows=1)  # Read only header\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path, nrows=1, engine=\"openpyxl\")  # Read only header\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    # Get actual column names\n",
        "    all_columns = df.columns.tolist()\n",
        "\n",
        "\n",
        "    des=[col for col in all_columns if any(re.search(keyword, col, re.IGNORECASE) for keyword in keywords)]\n",
        "    return des\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#anonymizing descriptive values\n",
        "#-----------------------------------------------------------------\n",
        "def detect_noun_desc(text):\n",
        "    doc = nlp(text)\n",
        "    modified_text=[]\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['PROPN']:\n",
        "            modified_text.append(\"<sensitive>\")\n",
        "        else:\n",
        "            modified_text.append(token.text)\n",
        "    text=\" \".join(modified_text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "#function for detecting numerical sensitive\n",
        "#------------------------------------------------------------------\n",
        "def detect_sensitive_numerical(file_path,sensitive):\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    sensitive_columns = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col not in sensitive:\n",
        "          text_samples = df[col].astype(str).head(5)  # Convert first 5 values to string\n",
        "\n",
        "          for value in text_samples:\n",
        "              if pd.notna(value) and value.strip():\n",
        "                  results = analyzer.analyze(text=value, language=\"en\")\n",
        "\n",
        "                  for result in results:\n",
        "                      # print(result,result.entity_type)\n",
        "                      if result.entity_type in [\"PHONE_NUMBER\", \"CREDIT_CARD\", \"IBAN\", \"US_SSN\",\"EMAIL\"]:\n",
        "                          sensitive_columns.append(col)\n",
        "                          break  # If any value in the column is sensitive, mark the whole column\n",
        "\n",
        "    return list(set(sensitive_columns))\n",
        "#-------------------------------------------------------------------\n",
        "#function to anonymize a excel file\n",
        "#-------------------------------------------------------------------\n",
        "def excel_an(input_file, output_file):\n",
        "    df = pd.read_excel(input_file)  # Read as string for safety\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    desc = descriptive_columns(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(desc))\n",
        "    num_sensitive=detect_sensitive_numerical(input_file,sensitive_old)\n",
        "    sensitive=list(set(sensitive)+set(num_sensitive))\n",
        "\n",
        "    column_counters = {col: 1 for col in df.columns if col in sensitive_old}\n",
        "    column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "    mapping = {}\n",
        "\n",
        "    # Anonymize sensitive columns while maintaining consistency\n",
        "    for col in sensitive:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]  # Use existing mapping\n",
        "                else:\n",
        "                    anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "                    column_mappings[col][val] = anonymized_value  # Store new mapping\n",
        "                    column_counters[col] += 1  # Increment counter\n",
        "\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "\n",
        "        df[col] = new_values\n",
        "\n",
        "    # Anonymize descriptive columns\n",
        "    for col in desc:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                an_values = detect_noun_desc(val)\n",
        "                df.at[idx, col] = an_values\n",
        "                mapping[an_values] = val\n",
        "\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"Anonymized Data\")\n",
        "    print(f\"‚úÖ Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "#------------------------------------------------------\n",
        "#function for de-anonymizing excel data\n",
        "#------------------------------------------------------\n",
        "\n",
        "def excel_dean(input_file, output_file, mapping_file):\n",
        "    print(\"üîÑ Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    df = pd.read_excel(input_file)  # Read as string for safety\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"De-anonymized Data\")\n",
        "\n",
        "    print(f\"‚úÖ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "#----------------------------------------------------------\n",
        "#function for anonymizing csv data\n",
        "#----------------------------------------------------------\n",
        "def csv_an(input_file, output_file):\n",
        "    df = pd.read_csv(input_file, engine=\"python\")\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    desc = descriptive_columns(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(desc))\n",
        "    num_sensitive=detect_sensitive_numerical(input_file,sensitive_old)\n",
        "    sensitive=sensitive+num_sensitive\n",
        "    column_counters = {col: 1 for col in df.columns if col in sensitive}\n",
        "    column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "\n",
        "    # Anonymize sensitive columns while maintaining consistency\n",
        "    for col in sensitive:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]  # Use existing mapping\n",
        "                else:\n",
        "                    anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "                    column_mappings[col][val] = anonymized_value  # Store new mapping\n",
        "                    column_counters[col] += 1  # Increment counter\n",
        "\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "\n",
        "        df[col] = new_values\n",
        "\n",
        "    # Anonymize descriptive columns\n",
        "    for col in desc:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                an_values = detect_noun_desc(val)\n",
        "                df.at[idx, col] = an_values\n",
        "                mapping[an_values] = val\n",
        "\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"‚úÖ Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "#------------------------------------------------------------\n",
        "#function for de anonymizing csv data\n",
        "#------------------------------------------------------------\n",
        "def csv_dean(input_file, output_file, mapping_file):\n",
        "    print(\"üîÑ Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "    df = pd.read_csv(input_file, engine=\"python\", dtype=str)  # Read as string for safety\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"‚úÖ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "#function to determine whcih type of data is need to perform\n",
        "#--------------------------------------------------------------\n",
        "def anonymization(input_file):\n",
        "    if input_file.endswith(\".csv\"):\n",
        "        csv_an(input_file,\"intermediate.csv\")\n",
        "        csv_dean(\"intermediate.csv\",\"deanonymized.csv\",\"mappings.json\")\n",
        "    elif input_file.endswith(\".xlsx\"):\n",
        "        excel_an(input_file,\"intermediate.xlsx\")\n",
        "        excel_dean(\"intermediate.xlsx\",\"deanonymized.xlsx\",\"mappings.json\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1bSCFFxznhj5",
        "outputId": "06ab4610-dacb-42f5-905c-96b52676df98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file=\"numerical.csv\"\n",
        "# anonymization(file)"
      ],
      "metadata": {
        "id": "mDhYiVpaRWRa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "5m6C2FgPcPa5",
        "outputId": "bc8009db-05e3-480c-8fdc-059174c328be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Downloading faker-37.0.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "import logging\n",
        "\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "\n",
        "from faker import Faker\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.CRITICAL, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "class MaskingModule:\n",
        "\n",
        "    def __init__(self, mapping_file: str = 'output_xlsx/data_mapping.json', seed: int = 42):\n",
        "\n",
        "        self.fake = Faker()\n",
        "\n",
        "        Faker.seed(seed)\n",
        "\n",
        "        self.mapping_file = mapping_file\n",
        "\n",
        "        self.forward_mapping = {}\n",
        "\n",
        "        self.reverse_mapping = {}\n",
        "\n",
        "        self._load_mapping_if_exists()\n",
        "\n",
        "        logger.info(\"‚úÖ ExcelDataMasker initialized.\")\n",
        "\n",
        "\n",
        "\n",
        "    def _generate_fake_value(self, original: str, column: str) -> str:\n",
        "\n",
        "        if isinstance(original, (int, float, np.integer, np.floating)):\n",
        "\n",
        "            return original\n",
        "\n",
        "\n",
        "\n",
        "        original_str = str(original)\n",
        "\n",
        "        if original_str in self.forward_mapping:\n",
        "\n",
        "            return self.forward_mapping[original_str]\n",
        "\n",
        "\n",
        "\n",
        "        column_lower = column.lower()\n",
        "\n",
        "        unique_fake_values = set(self.forward_mapping.values())\n",
        "\n",
        "\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if 'employee' in column_lower or 'company' in column_lower or 'account' in column_lower:\n",
        "\n",
        "                fake_value = f\"{self.fake.first_name()}{self.fake.random_number(digits=12)}{self.fake.first_name()}\"\n",
        "\n",
        "            elif 'project' in column_lower:\n",
        "\n",
        "                fake_value = f\"PRJ-{self.fake.random_number(digits=12)}\"\n",
        "\n",
        "            elif 'entity' in column_lower:\n",
        "\n",
        "                fake_value = f\"ENT-{self.fake.random_number(digits=12)}\"\n",
        "\n",
        "            elif 'program' in column_lower:\n",
        "\n",
        "                fake_value = f\"PROG-{self.fake.random_number(digits=12)}\"\n",
        "\n",
        "            else:\n",
        "\n",
        "                fake_value = f\"ANON-{self.fake.random_number(digits=12)}\"\n",
        "\n",
        "\n",
        "\n",
        "            if fake_value not in unique_fake_values:\n",
        "\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "        self.forward_mapping[original_str] = fake_value\n",
        "\n",
        "        self.reverse_mapping[fake_value] = original_str\n",
        "\n",
        "        logger.info(f\"üîÑ Mapping created: {original_str} ‚Üí {fake_value}\")\n",
        "\n",
        "        return fake_value\n",
        "\n",
        "\n",
        "\n",
        "    def _save_mapping(self):\n",
        "\n",
        "        mapping_data = {\n",
        "\n",
        "            'forward_mapping': self.forward_mapping,\n",
        "\n",
        "            'reverse_mapping': self.reverse_mapping,\n",
        "\n",
        "            'metadata': {\n",
        "\n",
        "                'updated_at': datetime.now().isoformat(),\n",
        "\n",
        "                'record_count': len(self.forward_mapping)\n",
        "\n",
        "            }\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        with open(self.mapping_file, 'w') as f:\n",
        "\n",
        "            json.dump(mapping_data, f, indent=4)\n",
        "\n",
        "        logger.info(\"üíæ Mapping saved successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "    def _load_mapping_if_exists(self):\n",
        "\n",
        "        if os.path.exists(self.mapping_file):\n",
        "\n",
        "            with open(self.mapping_file, 'r') as f:\n",
        "\n",
        "                data = json.load(f)\n",
        "\n",
        "                self.forward_mapping = data['forward_mapping']\n",
        "\n",
        "                self.reverse_mapping = data['reverse_mapping']\n",
        "\n",
        "            logger.info(\"üìÇ Existing mapping loaded.\")\n",
        "\n",
        "\n",
        "\n",
        "    def process_excel(self, input_file: str, output_file: str, mode: str = 'anonymize', include_columns: List[str] = None):\n",
        "\n",
        "        if mode not in ['anonymize', 'deanonymize']:\n",
        "\n",
        "            raise ValueError(\"Mode must be either 'anonymize' or 'deanonymize'\")\n",
        "\n",
        "\n",
        "\n",
        "        df = pd.read_excel(input_file)\n",
        "\n",
        "        include_set = {'Account', 'Project ID', 'Program Name', 'Employee', 'Cost Category', 'Entity', 'Description'}\n",
        "\n",
        "\n",
        "\n",
        "        if include_columns:\n",
        "\n",
        "            include_set.update(include_columns)\n",
        "\n",
        "\n",
        "\n",
        "        for column in df.columns:\n",
        "\n",
        "            if column in include_set:\n",
        "\n",
        "                if mode == 'anonymize':\n",
        "\n",
        "                    df[column] = df[column].apply(lambda x: self._generate_fake_value(x, column) if pd.notna(x) else x)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    df[column] = df[column].apply(lambda x: self.reverse_mapping.get(str(x), x) if pd.notna(x) and isinstance(x, str) else x)\n",
        "\n",
        "\n",
        "\n",
        "        df.to_excel(output_file, index=False)\n",
        "\n",
        "\n",
        "\n",
        "        if mode == 'anonymize':\n",
        "\n",
        "            self._save_mapping()\n",
        "\n",
        "\n",
        "\n",
        "        logger.info(f\"‚úÖ Data {mode}d and saved to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "    def check_if_two_xlsx_is_same(self, input_file: str, restored_file: str):\n",
        "\n",
        "        original_df = pd.read_excel(input_file)\n",
        "\n",
        "        restored_df = pd.read_excel(restored_file)\n",
        "\n",
        "        logger.info(\"üîé Verifying file integrity...\")\n",
        "\n",
        "        test = original_df.equals(restored_df)\n",
        "\n",
        "        logger.info(f\"üìä Are files identical? {test}\")\n",
        "\n",
        "\n",
        "\n",
        "        if not test:\n",
        "\n",
        "            logger.warning(\"‚ö†Ô∏è Files are not identical!\")\n",
        "\n",
        "            logger.info(\"üîç Sample from original:\")\n",
        "\n",
        "            logger.info(original_df.head(2))\n",
        "\n",
        "            logger.info(\"üîç Sample from restored:\")\n",
        "\n",
        "            logger.info(restored_df.head(2))\n",
        "\n",
        "        else:\n",
        "\n",
        "            logger.info(\"‚úÖ Files match perfectly!\")\n",
        "\n",
        "\n",
        "\n",
        "    def print_mapping_sample(self, n: int = 5):\n",
        "\n",
        "        logger.info(\"üìå Forward Mapping (Original ‚Üí Anonymized):\")\n",
        "\n",
        "        for i, (k, v) in enumerate(list(self.forward_mapping.items())[:n]):\n",
        "\n",
        "            logger.info(f\"{k} ‚Üí {v}\")\n",
        "\n",
        "\n",
        "\n",
        "        logger.info(\"üìå Reverse Mapping (Anonymized ‚Üí Original):\")\n",
        "\n",
        "        for i, (k, v) in enumerate(list(self.reverse_mapping.items())[:n]):\n",
        "\n",
        "            logger.info(f\"{k} ‚Üí {v}\")\n",
        "\n",
        "\n",
        "\n",
        "    def mask_text(self,text, replace_dict):\n",
        "\n",
        "        logger.info(\"üîÑ Processing text masking...\")\n",
        "\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        doc = nlp(text)\n",
        "\n",
        "\n",
        "\n",
        "        for key, value in replace_dict.items():\n",
        "\n",
        "            text = text.replace(key, value)\n",
        "\n",
        "\n",
        "\n",
        "        logger.info(\"‚úÖ Masking completed!\")\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    masker = MaskingModule()\n",
        "\n",
        "    input_file = '/content/split_file_part1.csv'\n",
        "\n",
        "    output_folder = \"output_xlsx\"\n",
        "\n",
        "\n",
        "\n",
        "    masker.process_excel(input_file=input_file, output_file=f'{output_folder}/anonymized.xlsx', mode='anonymize')\n",
        "\n",
        "    masker.print_mapping_sample()\n",
        "\n",
        "\n",
        "\n",
        "    logger.info(\"\\nStep 2: Deanonymize data...\")\n",
        "\n",
        "    masker.process_excel(input_file=f'{output_folder}/anonymized.xlsx', output_file=f'{output_folder}/restored.xlsx', mode='deanonymize')\n",
        "\n",
        "\n",
        "\n",
        "    masker.check_if_two_xlsx_is_same(input_file, f'{output_folder}/restored.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    mapping_file = 'output_xlsx/data_mapping.json'\n",
        "\n",
        "    text = \"\"\" Analysis complete for the query: ### üîç Overview:\n",
        "\n",
        "    The analysis reveals the average Seat Costs for 13 unique accounts in the cluster, ranging from $13.32 to $380.51 per seat, with an overall average of $156.39 across all accounts.\n",
        "\n",
        "\n",
        "\n",
        "    ### üìä Key Insights:\n",
        "\n",
        "    - üìà **Highest Seat Cost**: Account_002 has the highest average seat cost at $380.51.\n",
        "\n",
        "    - üìâ **Lowest Seat Cost**: Account_013 has the lowest average seat cost at $13.32.\n",
        "\n",
        "    - üèÜ **Top 3 Accounts by Seat Cost**:\n",
        "\n",
        "    1. Account_002 ($380.51)\n",
        "\n",
        "    2. Account_007 ($342.96)\n",
        "\n",
        "    3. Account_001 ($290.59)\n",
        "\n",
        "    - üìä **Data Quality**: No NaN values were found in the 'Actuals' column, indicating complete data for this metric.\n",
        "\n",
        "\n",
        "\n",
        "    ### üìë Detailed Explanation:\n",
        "\n",
        "    The expense report data was analyzed to calculate the average Seat Costs for all accounts in the cluster. Here's a breakdown of the findings:\n",
        "\n",
        "\n",
        "\n",
        "    1. **Distribution of Seat Costs**:\n",
        "\n",
        "    - The average Seat Costs vary significantly across accounts, with a range of $367.19 between the highest and lowest.\n",
        "\n",
        "    - 5 out of 13 accounts (38.5%) have above-average Seat Costs.\n",
        "\n",
        "    - 8 out of 13 accounts (61.5%) have below-average Seat Costs.\n",
        "\n",
        "\n",
        "\n",
        "    2. **Cost Tiers**:\n",
        "\n",
        "    - High Cost Tier (>$300): Account_002, Account_007\n",
        "\n",
        "    - Medium Cost Tier ($150-$300): Account_001, Account_003, Account_004\n",
        "\n",
        "    - Low Cost Tier (<$150): The remaining 8 accounts\n",
        "\n",
        "\n",
        "\n",
        "    3. **Statistical Analysis**:\n",
        "\n",
        "    - Median Seat Cost: $114.22 (Account_011)\n",
        "\n",
        "    - The overall average ($156.39) is higher than the median, suggesting some high-cost outliers are pulling the average up.\n",
        "\n",
        "\n",
        "\n",
        "    4. **Data Reliability**:\n",
        "\n",
        "    - All 13 accounts have complete data for Seat Costs, with no NaN values reported.\n",
        "\n",
        "    - This suggests high data quality and reliability for this particular metric.\n",
        "\n",
        "\n",
        "\n",
        "    üìå **Next Steps**:\n",
        "\n",
        "    - Investigate the factors contributing to the high Seat Costs in Account_002 and Account_007.\n",
        "\n",
        "    - Analyze the cost-efficiency measures implemented by lower-cost accounts (e.g., Account_013) for potential best practices.\n",
        "\n",
        "    - Consider segmenting accounts into cost tiers for targeted cost management strategies.\n",
        "\n",
        "\n",
        "\n",
        "    This analysis provides a clear overview of Seat Costs across all accounts, highlighting significant variations that merit further investigation for cost optimization opportunities.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(mapping_file):\n",
        "\n",
        "        logger.info(\"üìÇ Loading mapping file...\")\n",
        "\n",
        "\n",
        "\n",
        "        with open(mapping_file, 'r') as f:\n",
        "\n",
        "            data = json.load(f)\n",
        "\n",
        "            forward_mapping = data['forward_mapping']\n",
        "\n",
        "            reverse_mapping = data['reverse_mapping']\n",
        "\n",
        "\n",
        "\n",
        "            logger.info(\"üîÑ Applying forward mapping...\")\n",
        "\n",
        "            masked_text = masker.mask_text(text, forward_mapping)\n",
        "\n",
        "            logger.info(f\"üîè Masked Text: {masked_text}\")\n",
        "\n",
        "\n",
        "\n",
        "            logger.info(\"======================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "            logger.info(\"üîÑ Applying reverse mapping...\")\n",
        "\n",
        "            de_encrypt = masker.mask_text(masked_text, reverse_mapping)\n",
        "\n",
        "            logger.info(f\"üîì Decrypted Text: {de_encrypt}\")\n",
        "\n",
        "            print(f\"üòº Match Status: {de_encrypt == text} üòº\")\n",
        "\n",
        "            logger.info(f\"üòº Match Status: {de_encrypt == text} üòº\")\n",
        "\n",
        "    else:\n",
        "\n",
        "        logger.error(\"‚ùå Mapping file not found!\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "sFTKCp31cHdp",
        "outputId": "96a5641b-93c1-480c-a193-d5731bdfcfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Excel file format cannot be determined, you must specify an engine manually.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ebdcfde7aebd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{output_folder}/anonymized.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'anonymize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_mapping_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ebdcfde7aebd>\u001b[0m in \u001b[0;36mprocess_excel\u001b[0;34m(self, input_file, output_file, mode, include_columns)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0minclude_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Account'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Project ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Program Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Employee'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cost Category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Entity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Description'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n\u001b[1;32m   1553\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1555\u001b[0m                         \u001b[0;34m\"Excel file format cannot be determined, you must specify \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                         \u001b[0;34m\"an engine manually.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Excel file format cannot be determined, you must specify an engine manually."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from faker import Faker\n",
        "\n",
        "logging.basicConfig(level=logging.CRITICAL, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MaskingModule:\n",
        "    def __init__(self, mapping_file: str = \"output_xlsx/data_mapping.json\", seed: int = 42):\n",
        "        self.fake = Faker()\n",
        "        Faker.seed(seed)\n",
        "        self.mapping_file = mapping_file\n",
        "        self.forward_mapping = {}\n",
        "        self.reverse_mapping = {}\n",
        "        self._load_mapping_if_exists()\n",
        "        logger.info(\"‚úÖ MaskingModule initialized.\")\n",
        "\n",
        "    def _generate_fake_value(self, original: str, column: str) -> str:\n",
        "        \"\"\"Generates fake values for sensitive columns while ensuring uniqueness.\"\"\"\n",
        "        if isinstance(original, (int, float, np.integer, np.floating)):\n",
        "            original_str = str(original)\n",
        "        else:\n",
        "            original_str = str(original).strip()\n",
        "\n",
        "        # Return already mapped values\n",
        "        if original_str in self.forward_mapping:\n",
        "            return self.forward_mapping[original_str]\n",
        "\n",
        "        column_lower = column.lower()\n",
        "        unique_fake_values = set(self.forward_mapping.values())\n",
        "\n",
        "        while True:\n",
        "            if any(keyword in column_lower for keyword in [\"employee\", \"company\", \"account\"]):\n",
        "                fake_value = f\"{self.fake.first_name()}{self.fake.random_number(digits=6)}\"\n",
        "            elif \"project\" in column_lower:\n",
        "                fake_value = f\"PRJ-{self.fake.random_number(digits=6)}\"\n",
        "            elif \"entity\" in column_lower:\n",
        "                fake_value = f\"ENT-{self.fake.random_number(digits=6)}\"\n",
        "            elif \"program\" in column_lower:\n",
        "                fake_value = f\"PROG-{self.fake.random_number(digits=6)}\"\n",
        "            elif original_str.isdigit():  # Handle pure numerical values\n",
        "                fake_value = self.fake.random_number(digits=len(original_str))\n",
        "            else:\n",
        "                fake_value = f\"ANON-{self.fake.random_number(digits=6)}\"\n",
        "\n",
        "            if fake_value not in unique_fake_values:\n",
        "                break\n",
        "\n",
        "        # Save to mapping\n",
        "        self.forward_mapping[original_str] = str(fake_value)\n",
        "        self.reverse_mapping[str(fake_value)] = original_str\n",
        "        logger.info(f\"üîÑ Mapping created: {original_str} ‚Üí {fake_value}\")\n",
        "        return str(fake_value)\n",
        "\n",
        "    def _save_mapping(self):\n",
        "        os.makedirs(os.path.dirname(self.mapping_file), exist_ok=True)\n",
        "        mapping_data = {\n",
        "            \"forward_mapping\": self.forward_mapping,\n",
        "            \"reverse_mapping\": self.reverse_mapping,\n",
        "            \"metadata\": {\"updated_at\": datetime.now().isoformat(), \"record_count\": len(self.forward_mapping)},\n",
        "        }\n",
        "        with open(self.mapping_file, \"w\") as f:\n",
        "            json.dump(mapping_data, f, indent=4)\n",
        "        logger.info(\"üíæ Mapping saved successfully.\")\n",
        "\n",
        "    def _load_mapping_if_exists(self):\n",
        "        if os.path.exists(self.mapping_file):\n",
        "            with open(self.mapping_file, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                self.forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                self.reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "            logger.info(\"üìÇ Existing mapping loaded.\")\n",
        "\n",
        "    def process_file(self, input_file: str, output_file: str, mode: str = \"anonymize\", include_columns: List[str] = None):\n",
        "        \"\"\"Handles CSV and Excel anonymization and deanonymization.\"\"\"\n",
        "        if mode not in [\"anonymize\", \"deanonymize\"]:\n",
        "            raise ValueError(\"Mode must be either 'anonymize' or 'deanonymize'\")\n",
        "\n",
        "        file_type = \"csv\" if input_file.endswith(\".csv\") else \"excel\"\n",
        "        df = pd.read_csv(input_file) if file_type == \"csv\" else pd.read_excel(input_file)\n",
        "\n",
        "        include_set = {\"Account\", \"Project ID\", \"Program Name\", \"Employee\", \"Cost Category\", \"Entity\", \"Description\"}\n",
        "        if include_columns:\n",
        "            include_set.update(include_columns)\n",
        "\n",
        "        for column in df.columns:\n",
        "            if column in include_set:\n",
        "                if mode == \"anonymize\":\n",
        "                    df[column] = df[column].apply(lambda x: self._generate_fake_value(x, column) if pd.notna(x) else x)\n",
        "                else:\n",
        "                    df[column] = df[column].apply(lambda x: self.reverse_mapping.get(str(x), x) if pd.notna(x) and isinstance(x, str) else x)\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        if file_type == \"csv\":\n",
        "            df.to_csv(output_file, index=False)\n",
        "        else:\n",
        "            df.to_excel(output_file, index=False)\n",
        "\n",
        "        if mode == \"anonymize\":\n",
        "            self._save_mapping()\n",
        "\n",
        "        logger.info(f\"‚úÖ Data {mode}d and saved to {output_file}\")\n",
        "\n",
        "    def check_if_two_files_are_same(self, input_file: str, restored_file: str):\n",
        "        \"\"\"Verifies if original and restored files are identical.\"\"\"\n",
        "        file_type = \"csv\" if input_file.endswith(\".csv\") else \"excel\"\n",
        "\n",
        "        original_df = pd.read_csv(input_file) if file_type == \"csv\" else pd.read_excel(input_file)\n",
        "        restored_df = pd.read_csv(restored_file) if file_type == \"csv\" else pd.read_excel(restored_file)\n",
        "\n",
        "        logger.info(\"üîé Verifying file integrity...\")\n",
        "        test = original_df.astype(str).equals(restored_df.astype(str))  # Normalize dtypes before comparing\n",
        "        logger.info(f\"üìä Are files identical? {test}\")\n",
        "\n",
        "        if not test:\n",
        "            logger.warning(\"‚ö†Ô∏è Files are not identical!\")\n",
        "            logger.info(\"üîç Sample from original:\")\n",
        "            logger.info(original_df.head(2))\n",
        "            logger.info(\"üîç Sample from restored:\")\n",
        "            logger.info(restored_df.head(2))\n",
        "        else:\n",
        "            logger.info(\"‚úÖ Files match perfectly!\")\n",
        "\n",
        "    def mask_text(self, text, replace_dict):\n",
        "        \"\"\"Applies text masking based on the forward mapping.\"\"\"\n",
        "        logger.info(\"üîÑ Processing text masking...\")\n",
        "        for key, value in replace_dict.items():\n",
        "            text = text.replace(key, value)\n",
        "        logger.info(\"‚úÖ Masking completed!\")\n",
        "        return text\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    masker = MaskingModule()\n",
        "    input_file = \"data.csv\"  # Change to your input file (CSV or Excel)\n",
        "    output_folder = \"output_xlsx\"\n",
        "\n",
        "    # Step 1: Anonymize\n",
        "    masker.process_file(input_file=input_file, output_file=f\"{output_folder}/anonymized.csv\", mode=\"anonymize\")\n",
        "\n",
        "    # Step 2: Deanonymize\n",
        "    masker.process_file(input_file=f\"{output_folder}/anonymized.csv\", output_file=f\"{output_folder}/restored.csv\", mode=\"deanonymize\")\n",
        "\n",
        "    # Verify file integrity\n",
        "    masker.check_if_two_files_are_same(input_file, f\"{output_folder}/restored.csv\")\n",
        "\n",
        "    # Mask text using saved mappings\n",
        "    if os.path.exists(masker.mapping_file):\n",
        "        with open(masker.mapping_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            forward_mapping = data[\"forward_mapping\"]\n",
        "            reverse_mapping = data[\"reverse_mapping\"]\n",
        "\n",
        "            sample_text = \"John works at Google Inc. in New York.\"\n",
        "            masked_text = masker.mask_text(sample_text, forward_mapping)\n",
        "            decrypted_text = masker.mask_text(masked_text, reverse_mapping)\n",
        "\n",
        "            print(f\"üòº Match Status: {decrypted_text == sample_text} üòº\")\n"
      ],
      "metadata": {
        "id": "CpDIOwkcd7zT",
        "outputId": "d1389acf-d704-4476-91a5-97c53848cf5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üòº Match Status: True üòº\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "\n",
        "logging.basicConfig(level=logging.CRITICAL, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "MAPPING_FILE = \"output_xlsx/data_mapping.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mappings from file if available.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        with open(MAPPING_FILE, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            forward_mapping = data.get(\"forward_mapping\", {})\n",
        "            reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "        logger.info(\"üìÇ Existing mapping loaded.\")\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(MAPPING_FILE), exist_ok=True)\n",
        "    mapping_data = {\n",
        "        \"forward_mapping\": forward_mapping,\n",
        "        \"reverse_mapping\": reverse_mapping,\n",
        "        \"metadata\": {\"updated_at\": datetime.now().isoformat(), \"record_count\": len(forward_mapping)},\n",
        "    }\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "    logger.info(\"üíæ Mapping saved successfully.\")\n",
        "\n",
        "def generate_fake_value(original: str, column: str) -> str:\n",
        "    \"\"\"Generates fake values while ensuring uniqueness.\"\"\"\n",
        "    if isinstance(original, (int, float, np.integer, np.floating)):\n",
        "        original_str = str(original)\n",
        "    else:\n",
        "        original_str = str(original).strip()\n",
        "\n",
        "    if original_str in forward_mapping:\n",
        "        return forward_mapping[original_str]\n",
        "\n",
        "    column_lower = column.lower()\n",
        "    unique_fake_values = set(forward_mapping.values())\n",
        "\n",
        "    while True:\n",
        "        if any(keyword in column_lower for keyword in [\"employee\", \"company\", \"account\"]):\n",
        "            fake_value = f\"{fake.first_name()}{fake.random_number(digits=6)}\"\n",
        "        elif \"project\" in column_lower:\n",
        "            fake_value = f\"PRJ-{fake.random_number(digits=6)}\"\n",
        "        elif \"entity\" in column_lower:\n",
        "            fake_value = f\"ENT-{fake.random_number(digits=6)}\"\n",
        "        elif \"program\" in column_lower:\n",
        "            fake_value = f\"PROG-{fake.random_number(digits=6)}\"\n",
        "        elif original_str.isdigit():\n",
        "            fake_value = fake.random_number(digits=len(original_str))\n",
        "        else:\n",
        "            fake_value = f\"ANON-{fake.random_number(digits=6)}\"\n",
        "\n",
        "        if fake_value not in unique_fake_values:\n",
        "            break\n",
        "\n",
        "    forward_mapping[original_str] = str(fake_value)\n",
        "    reverse_mapping[str(fake_value)] = original_str\n",
        "    return str(fake_value)\n",
        "\n",
        "def process_file(input_file: str, output_file: str, mode: str = \"anonymize\", include_columns: List[str] = None):\n",
        "    \"\"\"Handles CSV and Excel anonymization and deanonymization.\"\"\"\n",
        "    if mode not in [\"anonymize\", \"deanonymize\"]:\n",
        "        raise ValueError(\"Mode must be either 'anonymize' or 'deanonymize'\")\n",
        "\n",
        "    file_type = \"csv\" if input_file.endswith(\".csv\") else \"excel\"\n",
        "    df = pd.read_csv(input_file) if file_type == \"csv\" else pd.read_excel(input_file)\n",
        "\n",
        "    include_set = {\"Account\", \"Project ID\", \"Program Name\", \"Employee\", \"Cost Category\", \"Entity\", \"Description\"}\n",
        "    if include_columns:\n",
        "        include_set.update(include_columns)\n",
        "\n",
        "    for column in df.columns:\n",
        "        if column in include_set:\n",
        "            if mode == \"anonymize\":\n",
        "                df[column] = df[column].apply(lambda x: generate_fake_value(x, column) if pd.notna(x) else x)\n",
        "            else:\n",
        "                df[column] = df[column].apply(lambda x: reverse_mapping.get(str(x), x) if pd.notna(x) and isinstance(x, str) else x)\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    if file_type == \"csv\":\n",
        "        df.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        df.to_excel(output_file, index=False)\n",
        "\n",
        "    if mode == \"anonymize\":\n",
        "        save_mapping()\n",
        "\n",
        "def check_if_two_files_are_same(input_file: str, restored_file: str):\n",
        "    \"\"\"Verifies if original and restored files are identical.\"\"\"\n",
        "    file_type = \"csv\" if input_file.endswith(\".csv\") else \"excel\"\n",
        "    original_df = pd.read_csv(input_file) if file_type == \"csv\" else pd.read_excel(input_file)\n",
        "    restored_df = pd.read_csv(restored_file) if file_type == \"csv\" else pd.read_excel(restored_file)\n",
        "\n",
        "    test = original_df.astype(str).equals(restored_df.astype(str))\n",
        "    logger.info(f\"üìä Are files identical? {test}\")\n",
        "    return test\n",
        "\n",
        "def mask_text(text, replace_dict):\n",
        "    \"\"\"Applies text masking using the forward mapping.\"\"\"\n",
        "    for key, value in replace_dict.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    input_file = \"data.csv\"\n",
        "    output_folder = \"output_xlsx\"\n",
        "    process_file(input_file, f\"{output_folder}/anonymized.csv\", mode=\"anonymize\")\n",
        "    process_file(f\"{output_folder}/anonymized.csv\", f\"{output_folder}/restored.csv\", mode=\"deanonymize\")\n",
        "    check_if_two_files_are_same(input_file, f\"{output_folder}/restored.csv\")\n",
        "\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        with open(MAPPING_FILE, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            sample_text = \"John works at Google Inc. in New York.\"\n",
        "            masked_text = mask_text(sample_text, data[\"forward_mapping\"])\n",
        "            decrypted_text = mask_text(masked_text, data[\"reverse_mapping\"])\n",
        "            print(f\"üòº Match Status: {decrypted_text == sample_text} üòº\")\n"
      ],
      "metadata": {
        "id": "8z_YX9F9jzKL",
        "outputId": "9a72de81-5fa0-4378-b75b-830d65c86bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üòº Match Status: True üòº\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize Faker and Presidio\n",
        "fake = Faker()\n",
        "analyzer = AnalyzerEngine()\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "# Dictionary for mapping anonymized values\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "\n",
        "def generate_fake_value(entity_type: str) -> str:\n",
        "    \"\"\"Generates fake data based on detected entity type.\"\"\"\n",
        "    if entity_type == \"PERSON\":\n",
        "        return fake.name()\n",
        "    elif entity_type == \"GPE\":  # Geographic locations\n",
        "        return fake.city()\n",
        "    elif entity_type == \"ORG\":  # Organizations\n",
        "        return fake.company()\n",
        "    elif entity_type == \"EMAIL\":\n",
        "        return fake.email()\n",
        "    elif entity_type == \"PHONE_NUMBER\":\n",
        "        return fake.phone_number()\n",
        "    elif entity_type == \"CREDIT_CARD\":\n",
        "        return fake.credit_card_number()\n",
        "    elif entity_type == \"IBAN\":\n",
        "        return fake.iban()\n",
        "    elif entity_type == \"DATE_TIME\":\n",
        "        return fake.date()\n",
        "    else:\n",
        "        return fake.word()  # Default replacement\n",
        "\n",
        "\n",
        "def anonymize_text(text: str) -> str:\n",
        "    \"\"\"Anonymizes sensitive entities in a given text.\"\"\"\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return text\n",
        "\n",
        "    results = analyzer.analyze(text=text, entities=None, language=\"en\")\n",
        "    anonymized_text = text\n",
        "\n",
        "    for result in results:\n",
        "        entity_value = text[result.start:result.end]\n",
        "        if entity_value in forward_mapping:\n",
        "            fake_value = forward_mapping[entity_value]\n",
        "        else:\n",
        "            fake_value = generate_fake_value(result.entity_type)\n",
        "            forward_mapping[entity_value] = fake_value\n",
        "            reverse_mapping[fake_value] = entity_value\n",
        "\n",
        "        anonymized_text = anonymized_text.replace(entity_value, fake_value)\n",
        "\n",
        "    return anonymized_text\n",
        "\n",
        "\n",
        "def process_file(input_file: str, output_file: str, mode: str = \"anonymize\"):\n",
        "    \"\"\"Handles anonymization and deanonymization for CSV and Excel files.\"\"\"\n",
        "    file_type = \"csv\" if input_file.endswith(\".csv\") else \"excel\"\n",
        "    df = pd.read_csv(input_file) if file_type == \"csv\" else pd.read_excel(input_file)\n",
        "\n",
        "    for column in df.columns:\n",
        "        if mode == \"anonymize\":\n",
        "            df[column] = df[column].apply(lambda x: anonymize_text(str(x)) if pd.notna(x) else x)\n",
        "        else:  # Deanonymization\n",
        "            df[column] = df[column].apply(lambda x: reverse_mapping.get(str(x), x) if pd.notna(x) and isinstance(x, str) else x)\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    if file_type == \"csv\":\n",
        "        df.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        df.to_excel(output_file, index=False)\n",
        "\n",
        "    logger.info(f\"‚úÖ Data {mode}d and saved to {output_file}\")\n",
        "\n",
        "    if mode == \"anonymize\":\n",
        "        save_mapping(\"output_xlsx/data_mapping.json\")\n",
        "\n",
        "\n",
        "def save_mapping(mapping_file: str):\n",
        "    \"\"\"Saves the anonymization mapping.\"\"\"\n",
        "    mapping_data = {\n",
        "        \"forward_mapping\": forward_mapping,\n",
        "        \"reverse_mapping\": reverse_mapping,\n",
        "        \"metadata\": {\"updated_at\": datetime.now().isoformat(), \"record_count\": len(forward_mapping)},\n",
        "    }\n",
        "    with open(mapping_file, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "    logger.info(\"üíæ Mapping saved successfully.\")\n",
        "\n",
        "\n",
        "def load_mapping(mapping_file: str):\n",
        "    \"\"\"Loads an existing anonymization mapping.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(mapping_file):\n",
        "        with open(mapping_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            forward_mapping = data.get(\"forward_mapping\", {})\n",
        "            reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "        logger.info(\"üìÇ Existing mapping loaded.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"data.csv\"  # Change to your input file (CSV or Excel)\n",
        "    output_folder = \"output_xlsx\"\n",
        "    mapping_file = \"output_xlsx/data_mapping.json\"\n",
        "\n",
        "    load_mapping(mapping_file)\n",
        "\n",
        "    # Step 1: Anonymize\n",
        "    process_file(input_file=input_file, output_file=f\"{output_folder}/anonymized.csv\", mode=\"anonymize\")\n",
        "\n",
        "    # Step 2: Deanonymize\n",
        "    process_file(input_file=f\"{output_folder}/anonymized.csv\", output_file=f\"{output_folder}/restored.csv\", mode=\"deanonymize\")\n"
      ],
      "metadata": {
        "id": "d4dWVhcyk6_E",
        "outputId": "bd5820a8-524a-450a-b744-7c69c261e3fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install presidio_analyzer presidio_anonymizer"
      ],
      "metadata": {
        "id": "6eFUGtuKihvw",
        "outputId": "4733a019-a8e7-458a-bd78-86156e0c773e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: presidio_analyzer in /usr/local/lib/python3.11/dist-packages (2.2.358)\n",
            "Collecting presidio_anonymizer\n",
            "  Downloading presidio_anonymizer-2.2.358-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: phonenumbers<9.0.0,>=8.12 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (8.13.55)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (2024.11.6)\n",
            "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (3.8.4)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (5.1.3)\n",
            "Requirement already satisfied: cryptography<44.1 in /usr/local/lib/python3.11/dist-packages (from presidio_anonymizer) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44.1->presidio_anonymizer) (1.17.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.10)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.18.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44.1->presidio_anonymizer) (2.22)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.2)\n",
            "Downloading presidio_anonymizer-2.2.358-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: presidio_anonymizer\n",
            "Successfully installed presidio_anonymizer-2.2.358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import openpyxl\n",
        "import json\n",
        "from faker import Faker\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "# Initialize components\n",
        "analyzer = AnalyzerEngine()\n",
        "anonymizer = AnonymizerEngine()\n",
        "fake = Faker()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "mapping = {}\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "# Function to detect and anonymize sensitive data using Presidio\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "def anonymize_text(text):\n",
        "    results = analyzer.analyze(text=text, language=\"en\")\n",
        "    modified_text = text  # Start with the original text\n",
        "\n",
        "    for result in sorted(results, key=lambda r: r.start, reverse=True):  # Process in reverse to avoid index shifting\n",
        "        fake_value = \"\"\n",
        "        if result.entity_type == \"PERSON\":\n",
        "            fake_value = fake.name()\n",
        "        elif result.entity_type == \"GPE\":\n",
        "            fake_value = fake.city()\n",
        "        elif result.entity_type == \"ORG\":\n",
        "            fake_value = fake.company()\n",
        "        elif result.entity_type in [\"PHONE_NUMBER\", \"EMAIL\", \"CREDIT_CARD\", \"IBAN\", \"US_SSN\"]:\n",
        "            fake_value = fake.word()\n",
        "\n",
        "        mapping[fake_value] = modified_text[result.start:result.end]\n",
        "        modified_text = modified_text[:result.start] + fake_value + modified_text[result.end:]  # Replace entity with fake value\n",
        "\n",
        "    return modified_text\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "# Function to find descriptive text columns\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "def descriptive_columns(file_path):\n",
        "    keywords = [\"description\", \"remarks\", \"notes\", \"comments\", \"observations\", \"details\", \"summary\", \"explanation\", \"reviews\", \"feedback\"]\n",
        "\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, nrows=1)\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path, nrows=1, engine=\"openpyxl\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    return [col for col in df.columns if any(re.search(keyword, col, re.IGNORECASE) for keyword in keywords)]\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "# Function for anonymizing CSV data\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "def csv_an(input_file, output_file):\n",
        "    df = pd.read_csv(input_file, engine=\"python\")\n",
        "    desc_cols = descriptive_columns(input_file)\n",
        "\n",
        "    for col in df.columns:\n",
        "        df[col] = df[col].astype(str).apply(lambda x: anonymize_text(x) if pd.notna(x) else x)\n",
        "\n",
        "    df.to_csv(output_file, index=False)\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "    print(f\"‚úÖ Anonymized file saved as {output_file}\")\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "# Function for anonymizing Excel data\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "def excel_an(input_file, output_file):\n",
        "    df = pd.read_excel(input_file)\n",
        "    desc_cols = descriptive_columns(input_file)\n",
        "\n",
        "    for col in df.columns:\n",
        "        df[col] = df[col].astype(str).apply(lambda x: anonymize_text(x) if pd.notna(x) else x)\n",
        "\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"Anonymized Data\")\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "    print(f\"‚úÖ Anonymized file saved as {output_file}\")\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "# Function for de-anonymizing CSV data\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "def csv_dean(input_file, output_file, mapping_file):\n",
        "    print(\"üîÑ Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "    df = pd.read_csv(input_file, engine=\"python\", dtype=str)\n",
        "\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"‚úÖ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "# Function for de-anonymizing Excel data\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "def excel_dean(input_file, output_file, mapping_file):\n",
        "    print(\"üîÑ Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "    df = pd.read_excel(input_file)\n",
        "\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"De-anonymized Data\")\n",
        "    print(f\"‚úÖ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "# Function to determine file type and apply anonymization\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "def anonymization(input_file):\n",
        "    if input_file.endswith(\".csv\"):\n",
        "        csv_an(input_file, \"intermediate.csv\")\n",
        "        csv_dean(\"intermediate.csv\", \"deanonymized.csv\", \"mappings.json\")\n",
        "    elif input_file.endswith(\".xlsx\"):\n",
        "        excel_an(input_file, \"intermediate.xlsx\")\n",
        "        excel_dean(\"intermediate.xlsx\", \"deanonymized.xlsx\", \"mappings.json\")\n",
        "\n",
        "# Run anonymization\n",
        "file = \"numerical.csv\"\n",
        "anonymization(file)\n"
      ],
      "metadata": {
        "id": "X6k9AV1SpekJ",
        "outputId": "1a49106b-5ca9-4eb8-fc9b-b5005c76df30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Anonymized file saved as intermediate.csv\n",
            "üîÑ Loading data...\n",
            "‚úÖ De-anonymized file saved as deanonymized.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-934d8abe5db9>:96: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}